{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 37556,
     "status": "ok",
     "timestamp": 1721312737524,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "YM6Gf8B8O4wQ",
    "outputId": "1dd64b21-d575-44d2-a692-8fac65198206"
   },
   "outputs": [],
   "source": [
    "# Import all libraries and modules, define variables and objects needed to run the code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load progress idicator tools\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For loading dataframes by open link to google drive, to/from GC or GD\n",
    "import shutil\n",
    "\n",
    "# For ploting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For CatBoost modeling\n",
    "!pip install catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# For evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18971,
     "status": "ok",
     "timestamp": 1721312760629,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "qKkOJFZo2xqm",
    "outputId": "336d4689-0799-4c72-f5eb-fe0d43b4913e"
   },
   "outputs": [],
   "source": [
    "# If saving and loading files in GDrive is needed\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlEtlazy_4Nw"
   },
   "source": [
    "##**CatBoost model. Computation of new features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibvWxtg7e0B9"
   },
   "source": [
    "##**1. Computation of new features: load and read csv files with prepocessed data  and masks, compute new features**\n",
    "\n",
    "df1_ datasets should contain all the results after preprocessing ('PRICE_num', 'QUANTITY_num', 'HASHED_FRAUDRINGNAME_num', 'Label1').\n",
    "GPU is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HPrkMWb3CnN"
   },
   "outputs": [],
   "source": [
    "# 1.1 Read the previously preprocessed data from csv files\n",
    "\n",
    "df1_train_path = '...'\n",
    "df1_train = pd.read_csv(df1_train_path, delimiter = \",\", keep_default_na=False)\n",
    "\n",
    "df1_val_path = '...'\n",
    "df1_val = pd.read_csv(df1_val_path, delimiter = \",\", keep_default_na=False)\n",
    "\n",
    "df1_test_path = '...'\n",
    "df1_test = pd.read_csv(df1_test_path, delimiter = \",\", keep_default_na=False)\n",
    "\n",
    "\n",
    "# Get new variable DAY\n",
    "\n",
    "df1_train['DAY'] = pd.to_datetime(df1_train['_TRANSDATE']).dt.dayofweek\n",
    "df1_val['DAY'] = pd.to_datetime(df1_val['_TRANSDATE']).dt.dayofweek\n",
    "df1_test['DAY'] = pd.to_datetime(df1_test['_TRANSDATE']).dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnL0Esu-_wT_"
   },
   "outputs": [],
   "source": [
    "# Before switching to RAPID/cudf.pandas it worth to compute masks on GPU (due to a bug with idmax we didn't understand).\n",
    "# Alternativelly mask can be saved and loaded as csv files during RAPID session.\n",
    "\n",
    "# 1.2 Function to create masks for indeces when:\n",
    "# a) null values of PRICE and QUANTITY is droped\n",
    "# b) labels of Non-Fraud/Fraud transactions are only for Fraud\n",
    "# c) langauge of ITEM_NAMES is only English\n",
    "# d) for carts defining by max-price item\n",
    "# _mask_i - boolean mask for the a-c conditions (on items)\n",
    "# _mask_c - boolean mask for the a-d conditions (on carts)\n",
    "\n",
    "def masks():\n",
    "  global train_mask_i\n",
    "  global val_mask_i\n",
    "  global test_mask_i\n",
    "  global train_mask_c\n",
    "  global val_mask_c\n",
    "  global test_mask_c\n",
    "\n",
    "  #train masks\n",
    "  train_mask_i = (\n",
    "    (~df1_train['PRICE_num'].isnull()) &\n",
    "    (~df1_train['QUANTITY_num'].isnull()) &\n",
    "    (df1_train['Label1'] != -1) &\n",
    "    (df1_train['ENG'])\n",
    "  )\n",
    "  train_mask_c_idx = df1_train[train_mask_i].groupby('HASHED_SESSIONID')['PRICE_num'].idxmax()\n",
    "  train_mask_c = pd.Series(False, index=df1_train.index)\n",
    "  train_mask_c.iloc[train_mask_c_idx] = True\n",
    "\n",
    "  #val masks\n",
    "  val_mask_i = (\n",
    "    (~df1_val['PRICE_num'].isnull()) &\n",
    "    (~df1_val['QUANTITY_num'].isnull()) &\n",
    "    (df1_val['Label1'] != -1) &\n",
    "    (df1_val['ENG'])\n",
    "  )\n",
    "  val_mask_c_idx = df1_val[val_mask_i].groupby('HASHED_SESSIONID')['PRICE_num'].idxmax()\n",
    "  val_mask_c = pd.Series(False, index=df1_val.index)\n",
    "  val_mask_c.iloc[val_mask_c_idx] = True\n",
    "\n",
    "\n",
    "  #test masks\n",
    "  test_mask_i = (\n",
    "    (~df1_test['PRICE_num'].isnull()) &\n",
    "    (~df1_test['QUANTITY_num'].isnull()) &\n",
    "    (df1_test['Label1'] != -1) &\n",
    "    (df1_test['ENG'])\n",
    "  )\n",
    "  test_mask_c_idx = df1_test[test_mask_i].groupby('HASHED_SESSIONID')['PRICE_num'].idxmax()\n",
    "  test_mask_c = pd.Series(False, index=df1_test.index)\n",
    "  test_mask_c.iloc[test_mask_c_idx] = True\n",
    "\n",
    "  return\n",
    "\n",
    "masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 178537,
     "status": "ok",
     "timestamp": 1721029124245,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "afEzTL_hPUSx",
    "outputId": "3f3e56e2-6977-4626-dfd4-c95b0d62b98f"
   },
   "outputs": [],
   "source": [
    "# The RAPID & cudf.pandas were used for boosting computation of new lag probabilistic features on dataframes\n",
    "# It apeared that it worked quit fast and correct for some operations with pandas but for such one as idmax results were incorrect (maybe due to our errors in code)\n",
    "\n",
    "# The next text and code in this cell from NVIDIA\n",
    "\n",
    "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
    "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!python rapidsai-csp-utils/colab/pip-install.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VV-GgttSAyD2"
   },
   "outputs": [],
   "source": [
    "# For NVIDIA RAPIDS with cudf - to compute new features only\n",
    "import cudf\n",
    "#cudf.__version__\n",
    "%load_ext cudf.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaQjX5VW_5Nc"
   },
   "outputs": [],
   "source": [
    "# 1.3 The function to calculate lag Probabilities P(ItemX|site_id)\n",
    "\n",
    "def prob_day(df, dayX, item_name, site_id):\n",
    "    #d0 = (dayX)  # Calculation for yesterday (standing at the point 23:59 of today)\n",
    "    #d1 = (dayX - 1)  # Calculation for уesterday-1\n",
    "    #d2 = (dayX - 2)  # Calculation for уesterday-2\n",
    "\n",
    "  variables = {}\n",
    "\n",
    "  for idx, d in enumerate(range(dayX, dayX-3, -1)):\n",
    "    date_mask = (df['DAY'] == d)\n",
    "    item_mask = (df['ITEM_NAME'] == item_name)\n",
    "    site_mask = (df['HASHED_SITEID'] == site_id)\n",
    "\n",
    "    mask_N_itemXsiteX = date_mask & item_mask & site_mask\n",
    "    mask_N_siteX = date_mask & site_mask\n",
    "\n",
    "    N_item_site = df.loc[mask_N_itemXsiteX]['QUANTITY_num'].sum()\n",
    "    N_site = df.loc[mask_N_siteX]['QUANTITY_num'].sum()\n",
    "\n",
    "    variables[f'P_siteX_day{idx}'] = N_item_site / N_site if N_site > 1e-6 else 0\n",
    "\n",
    "  return variables['P_siteX_day0'], variables['P_siteX_day1'], variables['P_siteX_day2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w45t0QJAC_Kd"
   },
   "outputs": [],
   "source": [
    "# 1.4 the function to create new lag features in dataframes\n",
    "\n",
    "def get_prob_features(df, df1_mask, class1ratio):\n",
    "    if not all(col in df.columns for col in ['_TRANSDATE', 'ITEM_NAME', 'HASHED_SITEID', 'QUANTITY_num', 'Label1']):\n",
    "        raise KeyError(\"One or more required columns are missing from the DataFrame\")\n",
    "\n",
    "    global downsampled_indices\n",
    "    global mindate\n",
    "    global df_downsampled\n",
    "    global days\n",
    "\n",
    "    df['DAY'] = pd.to_datetime(df['_TRANSDATE']).dt.day\n",
    "\n",
    "    # Mask for downsampled the train set - to balance RF and FR\n",
    "    class_1_indices = df[df1_mask].loc[df['Label1'] == 1].index  # RF\n",
    "    class_2_indices = df[df1_mask].loc[df['Label1'] == 2].index  # FR\n",
    "    np.random.seed(42)\n",
    "    downsampled_class_1_indices = np.random.choice(class_1_indices, size=int(class1ratio*len(class_2_indices)), replace=False)\n",
    "    downsampled_indices = np.concatenate([downsampled_class_1_indices, class_2_indices])\n",
    "\n",
    "    mindate = min(df.loc[downsampled_indices, 'DAY']) + 2\n",
    "\n",
    "    tqdm.pandas()\n",
    "\n",
    "    # Extract relevant columns for downsampled indices\n",
    "    df_downsampled = df.loc[downsampled_indices, ['DAY', 'ITEM_NAME', 'HASHED_SITEID']]\n",
    "\n",
    "    # Select the day for each transaction date\n",
    "    days = df_downsampled['DAY']\n",
    "\n",
    "    # Initialize arrays to store probabilities\n",
    "    P_siteX_day0 = np.zeros(len(df_downsampled))\n",
    "    P_siteX_day1 = np.zeros(len(df_downsampled))\n",
    "    P_siteX_day2 = np.zeros(len(df_downsampled))\n",
    "\n",
    "    #breakpoint()\n",
    "\n",
    "    # Iterate over the downsampled indices\n",
    "    for i, (index, row) in enumerate(tqdm(df_downsampled.iterrows(), total=len(df_downsampled))):\n",
    "\n",
    "        if row['DAY'] > mindate:\n",
    "            P_siteX_day0[i], P_siteX_day1[i], P_siteX_day2[i] = prob_day(df, days.iloc[i], row['ITEM_NAME'], row['HASHED_SITEID'])\n",
    "\n",
    "        else:\n",
    "            P_siteX_day0[i], P_siteX_day1[i], P_siteX_day2[i] = np.nan, np.nan, np.nan\n",
    "\n",
    "    # Assign the calculated probabilities back to the dataframe\n",
    "    df.loc[downsampled_indices, 'P_siteX_day0'] = P_siteX_day0\n",
    "    df.loc[downsampled_indices, 'P_siteX_day1'] = P_siteX_day1\n",
    "    df.loc[downsampled_indices, 'P_siteX_day2'] = P_siteX_day2\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2820774,
     "status": "ok",
     "timestamp": 1721013533799,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "2Pka5ijL3mew",
    "outputId": "11d64bf8-8268-4fd6-bda8-64f302ffabe4"
   },
   "outputs": [],
   "source": [
    "# 1.5 Get new features.\n",
    "\n",
    "train_mask = train_mask_c\n",
    "class1ratio = 100\n",
    "get_prob_features(df1_train, train_mask, class1ratio)\n",
    "\n",
    "val_mask = val_mask_c\n",
    "class1ratio = 100\n",
    "get_prob_features(df1_val,  val_mask, class1ratio)\n",
    "\n",
    "test_mask = test_mask_c\n",
    "class1ratio = 100\n",
    "get_prob_features(df1_test,  test_mask, class1ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 351,
     "status": "error",
     "timestamp": 1721013745657,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "mU1fxjOchkCs",
    "outputId": "b2ae435e-70a7-4a4f-e296-805c2d62c0c7"
   },
   "outputs": [],
   "source": [
    "# Save and load results to GDrive\n",
    "\n",
    "\n",
    "df_tosave = df1_train\n",
    "prefix_name = 'df1_train_newfeatures'\n",
    "\n",
    "filename = f'{prefix_name}.csv'\n",
    "df_tosave.to_csv(filename, index=False)\n",
    "\n",
    "source_path = f'/content/{filename}'\n",
    "destination_path = f'/content/drive/My Drive/....../{filename}'\n",
    "\n",
    "shutil.copy(source_path, destination_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzXJ5GSV_ive"
   },
   "source": [
    "##**2. Load csv files with df1, masks and new features, and npy datasets with embeddinds, concatinate the other variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04xVYrE9bBTO"
   },
   "outputs": [],
   "source": [
    "#2.1 Load df1 sets if they were saved on GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4XUymAwQ22I"
   },
   "outputs": [],
   "source": [
    "# 2.2 Load and read masks as csv files (if needed, no needs in case to train and predict fith P features - use 2.2 b))\n",
    "# a) null values of PRICE and QUANTITY is droped\n",
    "# b) labels of Non-Fraud/Fraud transactions are only for Fraud\n",
    "# c) langauge of ITEM_NAMES is only English\n",
    "# d) for carts defining by max-price item\n",
    "# _mask_i - boolean mask for the a-c conditions (on items)\n",
    "# _mask_c - boolean mask for the a-d conditions (on carts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1721312872842,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "KK1kuAc7p-ce",
    "outputId": "341d2265-03b6-4360-9f16-c1946ecdcae8"
   },
   "outputs": [],
   "source": [
    "# 2.2 b) Compute the masks to train and predict sets fith lag new features\n",
    "\n",
    "train_mask_cnew = df1_train['P_siteX_day0'].notna() & df1_train['P_siteX_day1'].notna() & df1_train['P_siteX_day2'].notna()\n",
    "\n",
    "val_mask_cnew = df1_val['P_siteX_day0'].notna() & df1_val['P_siteX_day1'].notna() & df1_val['P_siteX_day2'].notna()\n",
    "\n",
    "test_mask_cnew = df1_test['P_siteX_day0'].notna() & df1_test['P_siteX_day1'].notna() & df1_test['P_siteX_day2'].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "4ZXBsY5gNUp1"
   },
   "outputs": [],
   "source": [
    "# 2.3 Load and read the saved npy files with embeddings for futher procesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94363,
     "status": "ok",
     "timestamp": 1721313590242,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "4vVVXeDy3j4o",
    "outputId": "85032c27-9091-4250-f3b0-e99fe339268c"
   },
   "outputs": [],
   "source": [
    "# 2.4 Concatinate numpy datasets with 6 num features (including 3 lag features) and 3 cat features\n",
    "\n",
    "# train_df\n",
    "\n",
    "train_df = pd.DataFrame(train_embed, columns=[f'feature_{i}' for i in range(300)])\n",
    "plusnumeric_df = df1_train[['PRICE_num', 'QUANTITY_num', 'RISK_SCORE', 'P_siteX_day0', 'P_siteX_day1', 'P_siteX_day2']]\n",
    "categorical_df = df1_train[['HASHED_SITEID', 'PGROUP', 'DAY']]\n",
    "train_df = pd.concat([train_df, plusnumeric_df, categorical_df], axis=1)\n",
    "print(train_df.shape)\n",
    "\n",
    "# val_df\n",
    "val_df = pd.DataFrame(val_embed, columns=[f'feature_{i}' for i in range(300)])\n",
    "plusnumeric_df = df1_val[['PRICE_num', 'QUANTITY_num', 'RISK_SCORE', 'P_siteX_day0', 'P_siteX_day1', 'P_siteX_day2']]\n",
    "categorical_df = df1_val[['HASHED_SITEID', 'PGROUP', 'DAY']]\n",
    "val_df = pd.concat([val_df, plusnumeric_df, categorical_df], axis=1)\n",
    "print(val_df.shape)\n",
    "\n",
    "\n",
    "#test_df\n",
    "test_df = pd.DataFrame(test_embed, columns=[f'feature_{i}' for i in range(300)])\n",
    "plusnumeric_df = df1_test[['PRICE_num', 'QUANTITY_num', 'RISK_SCORE', 'P_siteX_day0', 'P_siteX_day1', 'P_siteX_day2']]\n",
    "categorical_df = df1_test[['HASHED_SITEID', 'PGROUP', 'DAY']]\n",
    "test_df = pd.concat([test_df, plusnumeric_df, categorical_df], axis=1)\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fMqaRa-Xyt3"
   },
   "source": [
    "## **3. Functions to train and validate the CatBoost model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QQo4HbONIQfh"
   },
   "outputs": [],
   "source": [
    "# 3.2 Function to train the CatBoost model\n",
    "\n",
    "def catboost_train_val(params, class1ratio, train_mask, val_mask): #, test_mask):\n",
    "\n",
    "    cat_features = params['cat_features']\n",
    "\n",
    "    X_train = train_df.loc[train_mask]\n",
    "    y_train = df1_train.loc[train_mask, 'Label1']\n",
    "    X_val = val_df.loc[val_mask]\n",
    "    y_val = df1_val.loc[val_mask, 'Label1']\n",
    "\n",
    "    bst = CatBoostClassifier(**params)\n",
    "    bst.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=10)\n",
    "\n",
    "    return bst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8795,
     "status": "ok",
     "timestamp": 1721313833376,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "XdJwUcC443Jd",
    "outputId": "3ed4d09d-0317-46cc-9622-976387925cd9"
   },
   "outputs": [],
   "source": [
    "# 3.4 Define parameters and train the model with masks taking into account only rows with the P features (P day0, P day1, P day2)\n",
    "\n",
    "class1ratio = 100\n",
    "\n",
    "train_mask = train_mask_cnew\n",
    "val_mask   = val_mask_cnew\n",
    "\n",
    "params = {\n",
    "    'cat_features': ['HASHED_SITEID', 'PGROUP', 'DAY'],\n",
    "    'iterations': 500,\n",
    "    'learning_rate': 0.1,\n",
    "    'depth': 6,\n",
    "    'verbose': 10, # iteration for output 1 peace of information about training\n",
    "    'scale_pos_weight': class1ratio,\n",
    "}\n",
    "\n",
    "bst = catboost_train_val(params, class1ratio, train_mask, val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JM63dbrB5ClP"
   },
   "outputs": [],
   "source": [
    "# 3.5 Predict with the trained model\n",
    "test_mask  = test_mask_cnew\n",
    "predicted_probabilities = bst.predict_proba(test_df[test_mask_cnew])\n",
    "\n",
    "#pred = bst.predict(test_df[test_mask_cnew]) # binary label prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvDHzqPU64Od"
   },
   "outputs": [],
   "source": [
    "# 3.6 Get and save features importance\n",
    "\n",
    "feature_importances = bst.get_feature_importance()\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': train_df.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "importance_df\n",
    "\n",
    "importance_df.to_csv('feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1721313861441,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "vcuzwzLLsHbn",
    "outputId": "802c2ead-c3d8-47a3-bdd6-e28452ada1b1"
   },
   "outputs": [],
   "source": [
    "# 3.7 Evaluation of the trained model: trained with balance 100:1 and using only rows with existing values of lag features, changing the threshold of confidence\n",
    "\n",
    "test_labels = df1_test[test_mask_cnew]['Label1'].values\n",
    "test_labels = (test_labels == 2).astype(int)\n",
    "preds_binary = (predicted_probabilities[:, 1] >= 0.9).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, preds_binary)\n",
    "precision = precision_score(test_labels, preds_binary)\n",
    "recall = recall_score(test_labels, preds_binary)\n",
    "f1 = f1_score(test_labels, preds_binary)\n",
    "conf_matrix = confusion_matrix(test_labels, preds_binary)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1721313895371,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "7jh85gqCIcC2",
    "outputId": "4e980a58-48b1-47b7-f0d7-ff556fa17945"
   },
   "outputs": [],
   "source": [
    "# 3.8 ROC-AUC analysis\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, predicted_probabilities[:, 1])\n",
    "\n",
    "# Compute area under the curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1870,
     "status": "ok",
     "timestamp": 1721316626752,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "tbaDDlG05z9X",
    "outputId": "a5d0bf66-c475-4586-df67-37c79467d733"
   },
   "outputs": [],
   "source": [
    "# Save the binary label to df1_test and get metrics distribution by PGROUP\n",
    "# Correct prediction of FR as FR - TP\n",
    "# Wrong prediction of RF as FR   - FP\n",
    "# Correct prediction of RF as RF - TN\n",
    "# Wrong prediction of FR as RF   - FN\n",
    "\n",
    "df1_test.loc[test_mask, 'FR_V1_100:1_309bin'] = preds_binary\n",
    "df_test_filtered = df1_test[test_mask]\n",
    "\n",
    "TP_mask = (df_test_filtered['Label1'] == 2) & (df_test_filtered['FR_V1_100:1_309bin'] == 1)\n",
    "FP_mask = (df_test_filtered['Label1'] == 1) & (df_test_filtered['FR_V1_100:1_309bin'] == 1)\n",
    "TN_mask = (df_test_filtered['Label1'] == 1) & (df_test_filtered['FR_V1_100:1_309bin'] == 0)\n",
    "FN_mask = (df_test_filtered['Label1'] == 2) & (df_test_filtered['FR_V1_100:1_309bin'] == 0)\n",
    "\n",
    "list_masks = {\n",
    "    'TP': TP_mask,\n",
    "    'FP': FP_mask,\n",
    "    'TN': TN_mask,\n",
    "    'FN': FN_mask\n",
    "}\n",
    "\n",
    "results = {mask_name: {} for mask_name in list_masks.keys()}\n",
    "purchases = {mask_name: {} for mask_name in list_masks.keys()}\n",
    "\n",
    "for vertical in df_test_filtered['PGROUP'].unique():\n",
    "    for mask_name, mask in list_masks.items():\n",
    "        num_cases = len(df_test_filtered[mask & (df_test_filtered['PGROUP'] == vertical)])\n",
    "        item_names = df_test_filtered[mask & (df_test_filtered['PGROUP'] == vertical)].groupby(['HASHED_SITEID'])['ITEM_NAME'].apply(list).reset_index()\n",
    "        print(item_names)\n",
    "\n",
    "        results[mask_name][vertical] = num_cases\n",
    "        purchases[mask_name][vertical] = item_names\n",
    "\n",
    "df_errors_vertical    = pd.DataFrame(results)\n",
    "#df_purchases_vertical = pd.DataFrame(purchases)\n",
    "\n",
    "df_errors_vertical['Precision'] = df_errors_vertical.apply(\n",
    "    lambda row: row['TP'] / (row['TP'] + row['FP']) if (row['TP'] + row['FP']) > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "df_errors_vertical['Recall'] = df_errors_vertical.apply(\n",
    "    lambda row: row['TP'] / (row['TP'] + row['FN']) if (row['TP'] + row['FN']) > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "new_columns_order = ['Precision', 'Recall'] + [col for col in df_errors_vertical.columns if col not in ['Precision', 'Recall']]\n",
    "df_errors_vertical = df_errors_vertical[new_columns_order]\n",
    "\n",
    "df_errors_vertical.to_csv('df_errors_FR.xlsx', index=True)\n",
    "\n",
    "purchases"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1Gti5-0qVBXBgYneBZ-_iO3JUR4GIboqw",
     "timestamp": 1721832277346
    },
    {
     "file_id": "1I0PdGS01MfCbiel9YWf-75UNNJy20_i-",
     "timestamp": 1720703804185
    },
    {
     "file_id": "1FJAWIChg4llasQrGtiAhMJlm-HsJbrNu",
     "timestamp": 1720345292631
    },
    {
     "file_id": "1tF3rH24V0CGTR_xAL46zbeF6zZqTlT4o",
     "timestamp": 1719908012404
    },
    {
     "file_id": "1NX3dRQN1ZxPAHzpz7Juxfz7uYSNs8Hf-",
     "timestamp": 1719231342124
    },
    {
     "file_id": "12CwLUZBJ2E4SNS6E_0iB0elQFkVEKGpk",
     "timestamp": 1717997881493
    },
    {
     "file_id": "1GZyuHqMn7wA8AfqS2JE0bfUGATOPFi61",
     "timestamp": 1716883237412
    },
    {
     "file_id": "1LCm0O1TwOFhYJVnQ0KOl_4jjsa2ufNvA",
     "timestamp": 1716285755980
    },
    {
     "file_id": "1sYNFKe2LdC_RqB9Qj2nLL3geECMuhXVP",
     "timestamp": 1714979754538
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
