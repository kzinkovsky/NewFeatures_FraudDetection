{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 13634,
     "status": "ok",
     "timestamp": 1719255940161,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "YM6Gf8B8O4wQ",
    "outputId": "c9c805f4-063a-4158-f494-a9701f18771f"
   },
   "outputs": [],
   "source": [
    "# Import all libraries and modules, define variables and objects needed to run the code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For loading dataframes to/from GColab or GDrive\n",
    "import gdown\n",
    "import shutil\n",
    "\n",
    "# Load progress idicator tools\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For cleaning text (from stopwords, CamelCase etc.) and changing type of obj variables to numeric\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('foam')\n",
    "\n",
    "# For label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# For vectorization with Spacy\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_lg\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg') # pre-trained model used for vektorization\n",
    "\n",
    "# For vectorization with BERT\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibvWxtg7e0B9"
   },
   "source": [
    "##**1. Load and read the initial CSV file with data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If google drive will be used as a storage for data and results\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 158951,
     "status": "ok",
     "timestamp": 1718004922316,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "yYsmv0LBo_p8",
    "outputId": "29fd32d5-9b7d-40f9-8293-25fa914c92c2"
   },
   "outputs": [],
   "source": [
    "# Load data file in GColab directory\n",
    "file_id = '...'\n",
    "gdown.download(id=file_id)\n",
    "\n",
    "# Read csv file as pandas df\n",
    "df_path = '...'\n",
    "df0 = pd.read_csv(df_path, delimiter = \",\", keep_default_na=False)\n",
    "df0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnrP1VbF4HWV"
   },
   "source": [
    "##**2. Split the main dataframe to train, validation and test dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 4137,
     "status": "ok",
     "timestamp": 1718006047488,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "JXVuS_ne_CTI",
    "outputId": "06f33cee-784d-4d70-ca2b-6dcbed1b277f"
   },
   "outputs": [],
   "source": [
    "# a) change type of _TRANSDATE to datetime\n",
    "df0['_TRANSDATE'] = pd.to_datetime(df0['_TRANSDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "mJyzZ2tm4D-e"
   },
   "outputs": [],
   "source": [
    "# b) split the main dataframe (df0) to 3 part - training (from 2023-06-01 to 2023-06-17),\n",
    "# validation (from 2023-06-18 to 2023-06-24) and testing (2023-06-25 to 2023-06-30)\n",
    "\n",
    "date1 = '2023-06-17'\n",
    "date2 = '2023-06-24'\n",
    "\n",
    "df1_train = df0.loc[df0['_TRANSDATE'] <= date1]\n",
    "df1_val = df0.loc[(df0['_TRANSDATE'] > date1) & (df0['_TRANSDATE'] <= date2)]\n",
    "df1_test = df0.loc[df0['_TRANSDATE'] > date2]\n",
    "\n",
    "# d) save df as csv for futher processing on demand\n",
    "df1_train.to_csv('df1_train.csv', index=False)\n",
    "df1_val.to_csv('df1_val.csv', index=False)\n",
    "df1_test.to_csv('df1_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6243,
     "status": "ok",
     "timestamp": 1718009039058,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "12KXuePnTS_3",
    "outputId": "6c7f6806-6857-450e-fb9b-76853cdc5bbc"
   },
   "outputs": [],
   "source": [
    "# check dataframes by time span\n",
    "print('df1_train:\\n', df1_train['_TRANSDATE'].value_counts().sort_index())\n",
    "print('df1_val:\\n', df1_val['_TRANSDATE'].value_counts().sort_index())\n",
    "print('df1_test:\\n', df1_test['_TRANSDATE'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nL3IANEVJqkg"
   },
   "source": [
    "##**3.1 Functions for preprocessing: language detection and NaN fore some variables droping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygqzvfNBJqEw"
   },
   "outputs": [],
   "source": [
    "def drop_nan(df, list_variables_dna):\n",
    "  return df.dropna(subset=list_variables_dna, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_O5-5DoB2iyk"
   },
   "outputs": [],
   "source": [
    "# Any language detection library can be used to determine the language of item names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TfpPA3SfXlM"
   },
   "source": [
    "##**3.2 Functions for preprocessing: ITEM_NAME cleaning function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EVXFq8bvfsJX"
   },
   "outputs": [],
   "source": [
    "# Cleaning functions: removes particular symbols, resolves CamelCases, deleats stopwords from the stopwords list\n",
    "\n",
    "def clean_ITEM(df, stopwords):\n",
    "    # Check if 'ITEM_NAME' column exists\n",
    "    if 'ITEM_NAME' not in df.columns:\n",
    "        return 'No ITEM_NAME column found'\n",
    "    else:\n",
    "        # Function to remove symbols and split camel case\n",
    "        def symbol_remover_camel_case_split(phrase):\n",
    "            # Remove all characters besides A-Za-z0-9\n",
    "            phrase = re.sub(\"[^A-Za-z0-9]\", \" \", phrase)\n",
    "            # Separate numbers from words at the end\n",
    "            phrase = re.sub('([a-z])([0-9]+)', r'\\1 \\2', phrase)\n",
    "            # Separate numbers from words at the beginning\n",
    "            phrase = re.sub('([0-9]+)([a-zA-Z])', r'\\1 \\2', phrase)\n",
    "            # Split camel case\n",
    "            phrase = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', phrase))\n",
    "            return phrase\n",
    "\n",
    "        # Function to remove stopwords\n",
    "        def remove_stopwords(item_name):\n",
    "            words = item_name.split()\n",
    "            cleaned_words = [word for word in words if word.lower() not in stopwords]\n",
    "            return ' '.join(cleaned_words)\n",
    "\n",
    "        # Apply symbol removal and camel case splitting\n",
    "        df['ITEM_NAME'] = df['ITEM_NAME'].apply(symbol_remover_camel_case_split)\n",
    "        # Apply stopwords removal\n",
    "        df['ITEM_NAME'] = df['ITEM_NAME'].apply(remove_stopwords)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb1nSuFGYr-S"
   },
   "source": [
    "##3.3 Function to change the obj variables to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwCQLA_FiEdm"
   },
   "outputs": [],
   "source": [
    "# To change type of variable to numeric\n",
    "\n",
    "def convert_to_float(value):\n",
    "    match = re.search(r'\\d+(\\.\\d+)?', str(value))\n",
    "    if match:\n",
    "        return float(match.group())\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3.4 Function to create labels on selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to create labels on selection\n",
    "\n",
    "def labels_create(conditions, choices):\n",
    "    return np.select(conditions, choices, default=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6eGJfAqJT6I"
   },
   "source": [
    "##**3.4 Function for embeddings: ITEM_NAME vectorization with Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2JYW4peG3_O"
   },
   "outputs": [],
   "source": [
    "# The function to transform ITEM_NAME to vector with Spacy with batches\n",
    "def batches_to_vectors_spacy(phrases, batch_size):\n",
    "    vectors = []\n",
    "    for i in tqdm(range(0, len(phrases), batch_size)):\n",
    "        batch = phrases[i:i+batch_size]\n",
    "        docs = nlp.pipe(batch)\n",
    "        vectors.extend([doc.vector for doc in docs]) # return as array\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3VkFkPSeXtO"
   },
   "source": [
    "##**4. Preprocessing, embeddings, and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1AO347RK80E"
   },
   "outputs": [],
   "source": [
    "# 4.3.1 a) Dropping NaN in Price and Quantity (it can be also done before training with matching masks)\n",
    "list_variables_dna = ['PRICE_num', 'QUANTITY_num']\n",
    "drop_nan(df1_train, list_variables_dna)\n",
    "drop_nan(df1_val, list_variables_dna)\n",
    "drop_nan(df1_test, list_variables_dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nm4fK4sK4lO"
   },
   "outputs": [],
   "source": [
    "# 4.3.1 b) Language detection.\n",
    "\n",
    "tqdm.pandas()\n",
    "df1_train['ENG'] = df1_train['ITEM_NAME'].progress_apply(lang_detector)\n",
    "df1_val['ENG'] = df1_val['ITEM_NAME'].progress_apply(lang_detector)\n",
    "df1_test['ENG'] = df1_test['ITEM_NAME'].progress_apply(lang_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRuddO9VmHnk"
   },
   "outputs": [],
   "source": [
    "# 4.3.2 Cleaning ITEM_NAME of data frames\n",
    "\n",
    "df1_train = clean_ITEM(df1_train, stopwords)\n",
    "df1_val = clean_ITEM(df1_val, stopwords)\n",
    "df1_test = clean_ITEM(df1_test, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gx_AOcoJgCDP"
   },
   "outputs": [],
   "source": [
    "# 4.3.3 a) Change PRICE, QUANTITY to numerical type\n",
    "\n",
    "df1_train['PRICE_num'] = df1_train['PRICE'].apply(convert_to_float).astype(np.float32)\n",
    "df1_train['QUANTITY_num'] = df1_train['QUANTITY'].apply(convert_to_float).astype(np.float32)\n",
    "\n",
    "df1_val['PRICE_num'] = df1_val['PRICE'].apply(convert_to_float).astype(np.float32)\n",
    "df1_val['QUANTITY_num'] = df1_val['QUANTITY'].apply(convert_to_float).astype(np.float32)\n",
    "\n",
    "df1_test['PRICE_num'] = df1_test['PRICE'].apply(convert_to_float).astype(np.float32)\n",
    "df1_test['QUANTITY_num'] = df1_test['QUANTITY'].apply(convert_to_float).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lQSK6FbXH25"
   },
   "outputs": [],
   "source": [
    "# 4.3.3 b) encoding HASHED_SITEID to categorical SITEID_num\n",
    "\n",
    "# concatinate HASHED_SITEID in all the df\n",
    "siteid_cc = pd.concat([df1_train['HASHED_SITEID'], df1_val['HASHED_SITEID'], df1_test['HASHED_SITEID']], axis=0)\n",
    "\n",
    "# LabelEncoder to transfer HASHED_SITEID to categorical data\n",
    "labeler = LabelEncoder()\n",
    "siteid_lab = labeler.fit_transform(siteid_cc)\n",
    "\n",
    "# Division back to df1_train, df1_val, df1_test to get absolutely identical encoding in a new SITEID_num\n",
    "df1_train['SITEID_num'] = siteid_lab[:len(df1_train)]\n",
    "df1_val['SITEID_num'] = siteid_lab[len(df1_train):(len(df1_train)+len(df1_val))]\n",
    "df1_test['SITEID_num'] = siteid_lab[(len(df1_train)+len(df1_val)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkDomL1Yzygo"
   },
   "source": [
    "**4.3.4 The following cell to do embeding of item names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JQ0a-ZfgAg_"
   },
   "outputs": [],
   "source": [
    "# Embedding of ITEM_NAME by Spacy\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "train_embed = batches_to_vectors_spacy(df1_train['ITEM_NAME'], batch_size)\n",
    "np.save('train_embed.npy', train_embed) # if files with embeddings will be used later\n",
    "\n",
    "val_embed = batches_to_vectors_spacy(df1_val['ITEM_NAME'], batch_size)\n",
    "np.save('val_embed.npy', val_embed) # if files with embeddings will be used later\n",
    "\n",
    "test_embed = batches_to_vectors_spacy(df1_test['ITEM_NAME'], batch_size)\n",
    "np.save('test_embed.npy', test_embed) # if files with embeddings will be used later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEWW9xWwVXd9"
   },
   "source": [
    "**4.3.5 Create labels: non-fraud (-1), regular fraud (1), fraud rings (2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SrkRInanhnQ"
   },
   "outputs": [],
   "source": [
    "# 4.3.5 a) Labels for df1_train\n",
    "\n",
    "# Convert HASHED_FRAUDRINGNAME to numeric type\n",
    "df1_train['HASHED_FRAUDRINGNAME_num'] = df1_train['HASHED_FRAUDRINGNAME'].apply(convert_to_float).astype(np.float32)\n",
    "\n",
    "# mask for conditions\n",
    "conditions_1 = [\n",
    "    (df1_train['IF_FRAUD'] == False),\n",
    "    ((df1_train['IF_FRAUD'] == True) & (df1_train['HASHED_FRAUDRINGNAME_num'].isna())),\n",
    "    (df1_train['HASHED_FRAUDRINGNAME_num'].notna())\n",
    "]\n",
    "\n",
    "# values of labels\n",
    "choices_1 = [-1, 1, 2]\n",
    "\n",
    "# call the function to create labels using the defined conditions and choises\n",
    "df1_train['Label1'] = labels_create(conditions_1, choices_1)\n",
    "\n",
    "# save results if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 24076,
     "status": "ok",
     "timestamp": 1718789375635,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "Puo9LalFeJUC",
    "outputId": "f244fd42-be6c-41cd-9acc-176921fdaaec"
   },
   "outputs": [],
   "source": [
    "# 4.3.5 b) Labels for df1_val\n",
    "\n",
    "# Convert HASHED_FRAUDRINGNAME to numeric type\n",
    "df1_val['HASHED_FRAUDRINGNAME_num'] = df1_val['HASHED_FRAUDRINGNAME'].apply(convert_to_float).astype(np.float32)\n",
    "\n",
    "# mask for conditions\n",
    "conditions_1 = [\n",
    "    (df1_val['IF_FRAUD'] == False),\n",
    "    ((df1_val['IF_FRAUD'] == True) & (df1_val['HASHED_FRAUDRINGNAME_num'].isna())),\n",
    "    (df1_val['HASHED_FRAUDRINGNAME_num'].notna())\n",
    "]\n",
    "\n",
    "# values of labels\n",
    "choices_1 = [-1, 1, 2]\n",
    "\n",
    "# call the function to create labels using the defined conditions and choises\n",
    "df1_val['Label1'] = labels_create(conditions_1, choices_1)\n",
    "\n",
    "# save results if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 31024,
     "status": "ok",
     "timestamp": 1718789450428,
     "user": {
      "displayName": "Kirill Zinkovsky",
      "userId": "03904820153479450417"
     },
     "user_tz": -180
    },
    "id": "28wY6Y94gcqC",
    "outputId": "9380ad49-3643-45e8-c9ff-bd3bc194f295"
   },
   "outputs": [],
   "source": [
    "# 4.3.5 c) Labels for df1_test\n",
    "\n",
    "# Convert HASHED_FRAUDRINGNAME to numeric type\n",
    "df1_test['HASHED_FRAUDRINGNAME_num'] = df1_test['HASHED_FRAUDRINGNAME'].apply(convert_to_float).astype(np.float32)\n",
    "\n",
    "# mask for conditions\n",
    "conditions_1 = [\n",
    "    (df1_test['IF_FRAUD'] == False),\n",
    "    ((df1_test['IF_FRAUD'] == True) & (df1_test['HASHED_FRAUDRINGNAME_num'].isna())),\n",
    "    (df1_test['HASHED_FRAUDRINGNAME_num'].notna())\n",
    "]\n",
    "\n",
    "# values of labels\n",
    "choices_1 = [-1, 1, 2]\n",
    "\n",
    "# call the function to create labels using the defined conditions and choises\n",
    "df1_test['Label1'] = labels_create(conditions_1, choices_1)\n",
    "\n",
    "# save results if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f5Phc6EKOdZ"
   },
   "source": [
    "All the results of preprocessing we usually saved as 3 csv and 3 npy files for futher work. Google Drive can be used as a storage and code like this can be used for mooving files from Colab to GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcyxTtUDL5Wm"
   },
   "outputs": [],
   "source": [
    "source_path = '...'\n",
    "destination_path = '....'\n",
    "shutil.copy(source_path, destination_path)\n",
    "\n",
    "source_path = '...'\n",
    "destination_path = '...'\n",
    "shutil.copy(source_path, destination_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1NX3dRQN1ZxPAHzpz7Juxfz7uYSNs8Hf-",
     "timestamp": 1721828346991
    },
    {
     "file_id": "12CwLUZBJ2E4SNS6E_0iB0elQFkVEKGpk",
     "timestamp": 1717997881493
    },
    {
     "file_id": "1GZyuHqMn7wA8AfqS2JE0bfUGATOPFi61",
     "timestamp": 1716883237412
    },
    {
     "file_id": "1LCm0O1TwOFhYJVnQ0KOl_4jjsa2ufNvA",
     "timestamp": 1716285755980
    },
    {
     "file_id": "1sYNFKe2LdC_RqB9Qj2nLL3geECMuhXVP",
     "timestamp": 1714979754538
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
